{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQxUFUPTbUXaQfRqNedm+Y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, lit, rand\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('DataEngineering').getOrCreate()\n",
        "\n",
        "# Define schema for the sales data\n",
        "schema = StructType([\n",
        "    StructField(\"order_id\", StringType(), True),\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"price\", DoubleType(), True)\n",
        "])\n",
        "# Generate base data\n",
        "data = [(\"O\" + str(i), \"P\" + str(101 + (i % 3)), i % 5 + 1, 10.0 + i * 0.1) for i in range(1000)]\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nHnHUopvsQH",
        "outputId": "e80c32c9-b100-48f6-bead-658cec516aad"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+-----+\n",
            "|order_id|product_id|quantity|price|\n",
            "+--------+----------+--------+-----+\n",
            "|      O0|      P101|       1| 10.0|\n",
            "|      O1|      P102|       2| 10.1|\n",
            "|      O2|      P103|       3| 10.2|\n",
            "|      O3|      P101|       4| 10.3|\n",
            "|      O4|      P102|       5| 10.4|\n",
            "|      O5|      P103|       1| 10.5|\n",
            "|      O6|      P101|       2| 10.6|\n",
            "|      O7|      P102|       3| 10.7|\n",
            "|      O8|      P103|       4| 10.8|\n",
            "|      O9|      P101|       5| 10.9|\n",
            "+--------+----------+--------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce skew for product_id 'P101'\n",
        "skewed_data = [(\"O\" + str(i), \"P101\", i % 5 + 1, 10.0 + i * 0.1) for i in range(1000, 20000)]\n",
        "skewed_df = spark.createDataFrame(skewed_data, schema)\n",
        "skewed_df.show(10)\n",
        "final_df = df.union(skewed_df)\n",
        "\n",
        "# Display the distribution to show the skew\n",
        "final_df.groupBy(\"product_id\").count().orderBy(col(\"count\").desc()).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytAF1Q7C81-S",
        "outputId": "4af12e3d-e3e3-49a7-d447-9d905887fb96"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+------------------+\n",
            "|order_id|product_id|quantity|             price|\n",
            "+--------+----------+--------+------------------+\n",
            "|   O1000|      P101|       1|             110.0|\n",
            "|   O1001|      P101|       2|110.10000000000001|\n",
            "|   O1002|      P101|       3|             110.2|\n",
            "|   O1003|      P101|       4|110.30000000000001|\n",
            "|   O1004|      P101|       5|             110.4|\n",
            "|   O1005|      P101|       1|             110.5|\n",
            "|   O1006|      P101|       2|110.60000000000001|\n",
            "|   O1007|      P101|       3|             110.7|\n",
            "|   O1008|      P101|       4|110.80000000000001|\n",
            "|   O1009|      P101|       5|             110.9|\n",
            "+--------+----------+--------+------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------+-----+\n",
            "|product_id|count|\n",
            "+----------+-----+\n",
            "|      P101|19334|\n",
            "|      P102|  333|\n",
            "|      P103|  333|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_source_path = \"/content/sample_data/skewed_data\"\n",
        "final_df.write.format(\"parquet\").mode(\"overwrite\").save(file_source_path)"
      ],
      "metadata": {
        "id": "fuDsa_p1-ibW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    sales_df = spark.read.format(\"parquet\").load(file_source_path)\n",
        "    print(\"Successfully read data from source path.\")\n",
        "    sales_df.printSchema()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading from S3: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBtlm2VjQyxU",
        "outputId": "1cf173cb-7cbd-46ca-d3a2-cbc2c9343a7f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully read data from source path.\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, broadcast, concat, lit, floor, rand, explode, array\n",
        "from pyspark.sql.types import IntegerType\n",
        "# This aggregation will be slow due to data skew.\n",
        "# In the Spark UI, you would see some tasks taking much longer than others.\n",
        "skewed_agg_df = sales_df.groupBy(\"product_id\").agg(count(\"order_id\").alias(\"total_orders\"))\n",
        "skewed_agg_df.orderBy(col(\"total_orders\").desc()).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRIOOLn2RLYv",
        "outputId": "35b8dfec-9a25-4f15-9698-9c00aaa49b28"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+\n",
            "|product_id|total_orders|\n",
            "+----------+------------+\n",
            "|      P101|       19334|\n",
            "|      P102|         333|\n",
            "|      P103|         333|\n",
            "+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the skewed key(s) from above result\n",
        "skew_keys = [\"P101\"]\n",
        "# Number of salted keys to generate\n",
        "salt_factor = 5\n",
        "# Create a salted DataFrame for the sales data\n",
        "salted_sales_df = sales_df.withColumn(\"salt\",\n",
        "    when(col(\"product_id\").isin(skew_keys), (rand() * salt_factor).cast(IntegerType()))\n",
        "    .otherwise(lit(0))\n",
        ").withColumn(\"salted_product_id\", concat(col(\"product_id\"), lit(\"_\"), col(\"salt\")))\n",
        "\n",
        "salted_agg_df = salted_sales_df.groupBy(\"salted_product_id\").agg(count(\"order_id\").alias(\"total_orders\"))\n",
        "salted_agg_df.show()\n",
        "\n",
        "# Remove the salt to get the final result\n",
        "processed_df = salted_agg_df.withColumn(\"product_id\",\n",
        "    when(col(\"salted_product_id\").contains(\"_\"), col(\"salted_product_id\").substr(lit(1), lit(4)))\n",
        "    .otherwise(col(\"salted_product_id\"))\n",
        ").groupBy(\"product_id\").agg(count(\"total_orders\").alias(\"total_orders\"))\n",
        "\n",
        "processed_df.orderBy(col(\"total_orders\").desc()).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAaCQ92FRuGk",
        "outputId": "9d26c620-2c5d-4bb0-87a3-808563eca192"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+------------+\n",
            "|salted_product_id|total_orders|\n",
            "+-----------------+------------+\n",
            "|           P101_1|        3986|\n",
            "|           P101_4|        3851|\n",
            "|           P101_2|        3762|\n",
            "|           P101_0|        3838|\n",
            "|           P101_3|        3897|\n",
            "|           P102_0|         333|\n",
            "|           P103_0|         333|\n",
            "+-----------------+------------+\n",
            "\n",
            "+----------+------------+\n",
            "|product_id|total_orders|\n",
            "+----------+------------+\n",
            "|      P101|           5|\n",
            "|      P102|           1|\n",
            "|      P103|           1|\n",
            "+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data skew handling methods:\n",
        "    # 1. Salting\n",
        "    lookup = df.select('skewed_key').distinct().limit(10)\n",
        "    salted_df = salted_join(df, lookup)\n",
        "    \n",
        "    # 2. Broadcast join\n",
        "    broadcasted_df = broadcast_join(df, lookup, 'skewed_key')\n",
        "    \n",
        "    # 3. Repartition\n",
        "    repartitioned_df = repartition_on_skewed_key(df, 'skewed_key')"
      ],
      "metadata": {
        "id": "q8TmOj9XVA09"
      }
    }
  ]
}